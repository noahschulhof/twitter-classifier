{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 10:46:17.390318: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-08 10:46:17.553352: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-08 10:46:18.363176: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 10:46:18.363246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 10:46:18.546289: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-08 10:46:18.903051: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-08 10:46:18.906608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-08 10:46:21.747017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/noahschulhof/.conda/envs/python_311_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2190584004</td>\n",
       "      <td>Tue Jun 16 03:08:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Vicki_Gee</td>\n",
       "      <td>i miss nikki nu nu already  shes always there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1881451988</td>\n",
       "      <td>Fri May 22 04:42:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PatCashin</td>\n",
       "      <td>So I had a dream last night. I  remember a sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2058252964</td>\n",
       "      <td>Sat Jun 06 14:34:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>deelectable</td>\n",
       "      <td>@girlyghost ohh poor sickly you   (((hugs)) ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2237307600</td>\n",
       "      <td>Fri Jun 19 05:34:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>justinekepa</td>\n",
       "      <td>it is raining again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2224301193</td>\n",
       "      <td>Thu Jun 18 09:20:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cmatt007</td>\n",
       "      <td>@MissKeriBaby wish I was in LA right now</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag         user  \\\n",
       "0       0  2190584004  Tue Jun 16 03:08:48 PDT 2009  NO_QUERY    Vicki_Gee   \n",
       "1       0  1881451988  Fri May 22 04:42:15 PDT 2009  NO_QUERY    PatCashin   \n",
       "2       0  2058252964  Sat Jun 06 14:34:17 PDT 2009  NO_QUERY  deelectable   \n",
       "3       0  2237307600  Fri Jun 19 05:34:22 PDT 2009  NO_QUERY  justinekepa   \n",
       "4       0  2224301193  Thu Jun 18 09:20:06 PDT 2009  NO_QUERY     cmatt007   \n",
       "\n",
       "                                                text  \n",
       "0  i miss nikki nu nu already  shes always there ...  \n",
       "1  So I had a dream last night. I  remember a sig...  \n",
       "2  @girlyghost ohh poor sickly you   (((hugs)) ho...  \n",
       "3                               it is raining again   \n",
       "4          @MissKeriBaby wish I was in LA right now   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('../tweets/tweets_10k.csv')\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "tweets = tweets.loc[np.random.choice(list(range(tweets.shape[0])), 1000, replace = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TweetTokenizer()\n",
    "\n",
    "def remove_handle(tweet):\n",
    "    tokens = t.tokenize(tweet)\n",
    "\n",
    "    if tokens[0].startswith('@'):\n",
    "        return ' '.join(tokens[1:])\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "tweets['cleaned'] = tweets['text'].apply(remove_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tweets['cleaned'], tweets['target'], test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = dbt(list(x_train), max_length = 30, truncation = True, padding = True)\n",
    "test_encodings = dbt(list(x_test), max_length = 30, truncation = True, padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((dict(train_encodings), list(y_train))).batch(2)\n",
    "test = tf.data.Dataset.from_tensor_slices((dict(test_encodings), list(y_test))).batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 137s 292ms/step - loss: 0.5974 - accuracy: 0.6725\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 115s 286ms/step - loss: 0.3325 - accuracy: 0.8587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa5282e2790>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = optimizers.Adam(learning_rate = 3e-5),\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train, batch_size = 2, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 30ms/step - loss: 0.2859 - accuracy: 0.9200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.28591465950012207, 'accuracy': 0.9200000166893005}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test, return_dict = True, batch_size = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
